{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Understanding Policies\n",
    "\n",
    "The **policy** is the heart of reinforcement learning - it's the agent's decision-making strategy. In this notebook, we'll explore different types of policies and understand what makes a good policy.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. What is a policy?\n",
    "2. Random policies (baseline)\n",
    "3. Linear policies (simple but effective)\n",
    "4. Neural network policies (deep RL)\n",
    "5. Comparing policy performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environments import InvertedPendulumEnv\n",
    "from src.policies import RandomPolicy, LinearPolicy, NeuralNetworkPolicy\n",
    "from src.utils import evaluate_policy, collect_episode, plot_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Policy?\n",
    "\n",
    "A policy $\\pi$ is a function that maps states to actions:\n",
    "\n",
    "$$a = \\pi(s)$$\n",
    "\n",
    "### Types of Policies\n",
    "\n",
    "**Deterministic**: Same state always gives same action\n",
    "- $\\pi(s) = a$\n",
    "\n",
    "**Stochastic**: Returns a probability distribution over actions\n",
    "- $\\pi(a|s) = P(a|s)$\n",
    "\n",
    "### What Makes a Good Policy?\n",
    "\n",
    "A good policy for the inverted pendulum should:\n",
    "1. Respond quickly to pole tilts\n",
    "2. Use appropriate force magnitude\n",
    "3. Consider both position and velocity\n",
    "4. Keep the cart centered (not just balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Policy (Baseline)\n",
    "\n",
    "The simplest policy - completely ignores the state and picks random actions.\n",
    "\n",
    "This serves as a **baseline** - any learning algorithm should beat random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random policy\n",
    "random_policy = RandomPolicy(action_low=-10.0, action_high=10.0)\n",
    "\n",
    "# See what it does\n",
    "state = np.array([0, 0, 0.1, 0])  # Leaning right\n",
    "\n",
    "print(\"Random Policy Actions:\")\n",
    "for i in range(5):\n",
    "    action = random_policy.get_action(state)\n",
    "    print(f\"  Action {i+1}: {action:.2f} N\")\n",
    "\n",
    "print(\"\\nNotice: Actions are random regardless of state!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the random policy\n",
    "env = InvertedPendulumEnv()\n",
    "result = evaluate_policy(env, random_policy, n_episodes=50, seed=42)\n",
    "\n",
    "print(\"Random Policy Performance:\")\n",
    "print(f\"  Mean reward: {result['mean_reward']:.1f} ± {result['std_reward']:.1f}\")\n",
    "print(f\"  Mean episode length: {result['mean_length']:.1f} steps\")\n",
    "print(f\"\\n  (Max possible: {env.max_steps} steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Policy\n",
    "\n",
    "A linear policy computes the action as a weighted sum of state features:\n",
    "\n",
    "$$a = w_1 x + w_2 \\dot{x} + w_3 \\theta + w_4 \\dot{\\theta} + b$$\n",
    "\n",
    "The weights $(w_1, w_2, w_3, w_4, b)$ are the **parameters** we can learn!\n",
    "\n",
    "### Why Linear?\n",
    "- Easy to understand and interpret\n",
    "- Fast to compute\n",
    "- Few parameters (easy to optimize)\n",
    "- Can be surprisingly effective!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear policy with zero weights\n",
    "linear_policy = LinearPolicy()\n",
    "\n",
    "print(\"Linear Policy Parameters:\")\n",
    "print(f\"  Weights: {linear_policy.weights}\")\n",
    "print(f\"  Bias: {linear_policy.bias}\")\n",
    "print(f\"  Total parameters: {linear_policy.get_num_params()}\")\n",
    "\n",
    "# Test with zero weights (will output zero)\n",
    "state = np.array([0, 0, 0.1, 0])\n",
    "action = linear_policy.get_action(state)\n",
    "print(f\"\\nAction for state {state}: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights manually based on our intuition:\n",
    "# - Positive theta (leaning right) -> positive force (push right)\n",
    "# - Positive theta_dot (falling right) -> even more positive force\n",
    "\n",
    "linear_policy.weights = np.array([0, 0, 10, 3])  # [x, x_dot, theta, theta_dot]\n",
    "linear_policy.bias = 0\n",
    "\n",
    "print(\"Manual weights: [x=0, x_dot=0, theta=10, theta_dot=3]\")\n",
    "print(\"\\nThis means:\")\n",
    "print(\"  - Ignore cart position and velocity\")\n",
    "print(\"  - action = 10*theta + 3*theta_dot\")\n",
    "\n",
    "# Test on different states\n",
    "test_states = [\n",
    "    np.array([0, 0, 0.1, 0]),   # Leaning right\n",
    "    np.array([0, 0, -0.1, 0]),  # Leaning left\n",
    "    np.array([0, 0, 0.1, 0.5]), # Falling right fast\n",
    "]\n",
    "\n",
    "print(\"\\nTest actions:\")\n",
    "for state in test_states:\n",
    "    action = linear_policy.get_action(state)\n",
    "    print(f\"  θ={state[2]:.2f}, θ̇={state[3]:.2f} -> action={action:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the manually-tuned linear policy\n",
    "result = evaluate_policy(env, linear_policy, n_episodes=50, seed=42)\n",
    "\n",
    "print(\"Linear Policy (manual weights) Performance:\")\n",
    "print(f\"  Mean reward: {result['mean_reward']:.1f} ± {result['std_reward']:.1f}\")\n",
    "print(f\"  Mean episode length: {result['mean_length']:.1f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize what the linear policy does\n",
    "from src.utils.visualization import plot_policy_surface\n",
    "\n",
    "fig = plot_policy_surface(\n",
    "    linear_policy,\n",
    "    state_ranges={'theta': (-0.3, 0.3), 'theta_dot': (-2, 2)},\n",
    "    fixed_states={'x': 0, 'x_dot': 0}\n",
    ")\n",
    "plt.suptitle('Linear Policy: Action vs (theta, theta_dot)', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and visualize an episode\n",
    "states, actions, rewards, info = collect_episode(env, linear_policy)\n",
    "history = {\n",
    "    'states': np.array(states),\n",
    "    'actions': np.array(actions),\n",
    "    'rewards': np.array(rewards)\n",
    "}\n",
    "\n",
    "fig = plot_trajectory(history, title=\"Linear Policy Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Policy\n",
    "\n",
    "A neural network can represent much more complex functions:\n",
    "\n",
    "$$a = \\text{NN}(s; \\theta)$$\n",
    "\n",
    "Where $\\theta$ are the network weights and biases.\n",
    "\n",
    "### Architecture\n",
    "- Input layer: 4 neurons (state)\n",
    "- Hidden layers: Configurable (e.g., 64 -> 64)\n",
    "- Output layer: 1 neuron (action)\n",
    "- Activation: ReLU for hidden, Tanh for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network policy\n",
    "nn_policy = NeuralNetworkPolicy(hidden_sizes=[64, 64])\n",
    "\n",
    "print(\"Neural Network Policy:\")\n",
    "print(nn_policy.get_network_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the neural network policy (with random weights)\n",
    "state = np.array([0, 0, 0.1, 0])\n",
    "action = nn_policy.get_action(state)\n",
    "\n",
    "print(f\"State: {state}\")\n",
    "print(f\"Action: {action:.3f}\")\n",
    "\n",
    "# It outputs different values for different states\n",
    "print(\"\\nActions for different states:\")\n",
    "for theta in [-0.2, -0.1, 0, 0.1, 0.2]:\n",
    "    state = np.array([0, 0, theta, 0])\n",
    "    action = nn_policy.get_action(state)\n",
    "    print(f\"  θ={theta:5.2f} -> action={action:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the untrained neural network\n",
    "result = evaluate_policy(env, nn_policy, n_episodes=50, seed=42)\n",
    "\n",
    "print(\"Neural Network Policy (untrained) Performance:\")\n",
    "print(f\"  Mean reward: {result['mean_reward']:.1f} ± {result['std_reward']:.1f}\")\n",
    "print(f\"  Mean episode length: {result['mean_length']:.1f} steps\")\n",
    "print(\"\\n  (Random weights - needs training!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the neural network policy surface\n",
    "fig = plot_policy_surface(\n",
    "    nn_policy,\n",
    "    state_ranges={'theta': (-0.3, 0.3), 'theta_dot': (-2, 2)},\n",
    "    fixed_states={'x': 0, 'x_dot': 0}\n",
    ")\n",
    "plt.suptitle('Neural Network Policy (untrained): Action vs (theta, theta_dot)', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Policies\n",
    "\n",
    "Let's compare all our policies side by side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all policies\n",
    "policies = {\n",
    "    'Random': RandomPolicy(),\n",
    "    'Linear (manual)': LinearPolicy(weights=np.array([0, 0, 10, 3])),\n",
    "    'Linear (better)': LinearPolicy(weights=np.array([-0.5, -1, 15, 5])),\n",
    "    'Neural Net': NeuralNetworkPolicy(hidden_sizes=[32, 32])\n",
    "}\n",
    "\n",
    "# Evaluate each\n",
    "results = {}\n",
    "n_episodes = 100\n",
    "\n",
    "print(f\"Evaluating policies over {n_episodes} episodes...\\n\")\n",
    "\n",
    "for name, policy in policies.items():\n",
    "    result = evaluate_policy(env, policy, n_episodes=n_episodes, seed=42)\n",
    "    results[name] = result\n",
    "    print(f\"{name:20s}: {result['mean_reward']:6.1f} ± {result['std_reward']:5.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "names = list(results.keys())\n",
    "means = [results[n]['mean_reward'] for n in names]\n",
    "stds = [results[n]['std_reward'] for n in names]\n",
    "lengths = [results[n]['mean_length'] for n in names]\n",
    "\n",
    "# Bar plot of mean rewards\n",
    "x = range(len(names))\n",
    "axes[0].bar(x, means, yerr=stds, capsize=5, color=['red', 'blue', 'green', 'purple'])\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(names, rotation=15)\n",
    "axes[0].set_ylabel('Mean Reward')\n",
    "axes[0].set_title('Policy Comparison')\n",
    "axes[0].axhline(y=env.max_steps, color='k', linestyle='--', label='Max possible')\n",
    "axes[0].legend()\n",
    "\n",
    "# Histogram of episode rewards for each policy\n",
    "for name in names:\n",
    "    axes[1].hist(results[name]['episode_rewards'], bins=20, alpha=0.5, label=name)\n",
    "axes[1].set_xlabel('Episode Reward')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Reward Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Policy Parameters\n",
    "\n",
    "Let's explore how changing parameters affects the linear policy's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different theta weights\n",
    "theta_weights = [2, 5, 10, 15, 20, 30]\n",
    "\n",
    "print(\"Effect of theta weight (with theta_dot=3):\\n\")\n",
    "\n",
    "performances = []\n",
    "for w_theta in theta_weights:\n",
    "    policy = LinearPolicy(weights=np.array([0, 0, w_theta, 3]))\n",
    "    result = evaluate_policy(env, policy, n_episodes=50, seed=42)\n",
    "    performances.append(result['mean_reward'])\n",
    "    print(f\"  w_theta={w_theta:2d}: {result['mean_reward']:6.1f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(theta_weights, performances, 'o-')\n",
    "plt.xlabel('Theta Weight')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Effect of Theta Weight on Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over theta and theta_dot weights\n",
    "theta_weights = [5, 10, 15, 20]\n",
    "theta_dot_weights = [1, 3, 5, 7]\n",
    "\n",
    "performance_grid = np.zeros((len(theta_weights), len(theta_dot_weights)))\n",
    "\n",
    "for i, w_theta in enumerate(theta_weights):\n",
    "    for j, w_theta_dot in enumerate(theta_dot_weights):\n",
    "        policy = LinearPolicy(weights=np.array([0, 0, w_theta, w_theta_dot]))\n",
    "        result = evaluate_policy(env, policy, n_episodes=30, seed=42)\n",
    "        performance_grid[i, j] = result['mean_reward']\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(performance_grid, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(label='Mean Reward')\n",
    "plt.xticks(range(len(theta_dot_weights)), theta_dot_weights)\n",
    "plt.yticks(range(len(theta_weights)), theta_weights)\n",
    "plt.xlabel('Theta_dot Weight')\n",
    "plt.ylabel('Theta Weight')\n",
    "plt.title('Linear Policy Performance Grid')\n",
    "\n",
    "# Mark the best\n",
    "best_idx = np.unravel_index(np.argmax(performance_grid), performance_grid.shape)\n",
    "plt.plot(best_idx[1], best_idx[0], 'r*', markersize=20)\n",
    "plt.show()\n",
    "\n",
    "best_theta = theta_weights[best_idx[0]]\n",
    "best_theta_dot = theta_dot_weights[best_idx[1]]\n",
    "print(f\"Best weights found: theta={best_theta}, theta_dot={best_theta_dot}\")\n",
    "print(f\"Performance: {performance_grid.max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Better Linear Policy\n",
    "Can you find a linear policy that achieves the maximum reward (500 steps) consistently?\n",
    "\n",
    "### Exercise 2: Include Position\n",
    "Modify the linear policy to also penalize cart position (keep it centered). What weights work?\n",
    "\n",
    "### Exercise 3: Compare Network Sizes\n",
    "Create neural networks with different sizes (e.g., [8], [32], [64, 64], [128, 128]). Do larger networks perform better with random weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Linear policy with position term\n",
    "# Try to keep the cart centered while balancing\n",
    "\n",
    "# Weights: [x, x_dot, theta, theta_dot]\n",
    "# Negative x weight: if cart is to the right (positive x), push left (negative action)\n",
    "\n",
    "policy_with_position = LinearPolicy(weights=np.array([-1, -0.5, 15, 5]))\n",
    "\n",
    "# Run an episode\n",
    "states, actions, rewards, info = collect_episode(env, policy_with_position)\n",
    "\n",
    "print(f\"Episode length: {len(rewards)}\")\n",
    "print(f\"Final cart position: {states[-1][0]:.2f} m\")\n",
    "\n",
    "# Plot the trajectory - look at cart position!\n",
    "history = {\n",
    "    'states': np.array(states),\n",
    "    'actions': np.array(actions),\n",
    "    'rewards': np.array(rewards)\n",
    "}\n",
    "fig = plot_trajectory(history, title=\"Linear Policy with Position Control\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "- **Policies** map states to actions\n",
    "- **Random policy**: Baseline for comparison\n",
    "- **Linear policy**: Simple, interpretable, few parameters\n",
    "- **Neural network policy**: Expressive, many parameters, needs training\n",
    "- **Parameter choice** significantly affects performance\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "The linear policy with manually-chosen weights already works well! This is because the inverted pendulum has relatively simple dynamics.\n",
    "\n",
    "But what if we don't know the right weights? That's where **learning** comes in!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll learn how to **train** policies automatically using reinforcement learning algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
