{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: Experiments and Visualization\n",
    "\n",
    "In this final notebook, we'll bring everything together! We'll run comprehensive experiments, create beautiful visualizations, and solidify our understanding of RL.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Running systematic experiments\n",
    "2. Visualizing trained policies\n",
    "3. Analyzing what the agent learned\n",
    "4. Creating animations\n",
    "5. Next steps in your RL journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from src.environments import InvertedPendulumEnv\n",
    "from src.policies import LinearPolicy, NeuralNetworkPolicy, RandomPolicy\n",
    "from src.utils import (\n",
    "    train_policy, evaluate_policy, collect_episode,\n",
    "    plot_trajectory, plot_training_progress, animate_pendulum\n",
    ")\n",
    "from src.utils.visualization import plot_phase_portrait, plot_policy_surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train the Best Policy\n",
    "\n",
    "Let's train a really good policy with enough iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with extra iterations\n",
    "env = InvertedPendulumEnv()\n",
    "best_policy = LinearPolicy()\n",
    "\n",
    "print(\"Training the best linear policy...\\n\")\n",
    "\n",
    "result = train_policy(\n",
    "    env, best_policy,\n",
    "    algorithm='evolutionary',\n",
    "    n_iterations=150,\n",
    "    population_size=30,\n",
    "    elite_frac=0.2,\n",
    "    noise_scale=0.3,\n",
    "    n_episodes_per_eval=10,\n",
    "    verbose=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal weights: {best_policy.get_flat_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thorough evaluation\n",
    "eval_result = evaluate_policy(env, best_policy, n_episodes=100, verbose=True)\n",
    "\n",
    "print(f\"\\nBest Linear Policy Performance:\")\n",
    "print(f\"  Mean reward: {eval_result['mean_reward']:.1f} Â± {eval_result['std_reward']:.1f}\")\n",
    "print(f\"  Mean length: {eval_result['mean_length']:.1f} steps\")\n",
    "print(f\"  Success rate: {100 * sum(r >= 490 for r in eval_result['episode_rewards']) / 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig = plot_training_progress(\n",
    "    result['reward_history'],\n",
    "    window_size=10\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize the Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random vs trained policy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "random_policy = RandomPolicy()\n",
    "\n",
    "for ax, (policy, title) in zip(axes, [\n",
    "    (random_policy, 'Random Policy'),\n",
    "    (best_policy, 'Trained Policy')\n",
    "]):\n",
    "    # Run episode\n",
    "    states, actions, rewards, _ = collect_episode(env, policy)\n",
    "    states = np.array(states)\n",
    "    \n",
    "    # Plot trajectory in state space\n",
    "    ax.plot(states[:, 2], states[:, 3], 'b-', alpha=0.7)\n",
    "    ax.plot(states[0, 2], states[0, 3], 'go', markersize=10, label='Start')\n",
    "    ax.plot(states[-1, 2], states[-1, 3], 'ro', markersize=10, label='End')\n",
    "    ax.set_xlabel('Pole Angle (rad)')\n",
    "    ax.set_ylabel('Angular Velocity (rad/s)')\n",
    "    ax.set_title(f'{title}\\n({len(states)} steps)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-0.3, 0.3)\n",
    "    ax.set_ylim(-2, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the policy surface\n",
    "fig = plot_policy_surface(\n",
    "    best_policy,\n",
    "    state_ranges={'theta': (-0.25, 0.25), 'theta_dot': (-2, 2)},\n",
    "    fixed_states={'x': 0, 'x_dot': 0},\n",
    "    resolution=100\n",
    ")\n",
    "plt.suptitle('Trained Linear Policy: Action = f(theta, theta_dot)', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Interpret the weights\n",
    "weights = best_policy.weights\n",
    "print(\"\\nLearned weights interpretation:\")\n",
    "print(f\"  x coefficient: {weights[0]:.3f}\")\n",
    "print(f\"  x_dot coefficient: {weights[1]:.3f}\")\n",
    "print(f\"  theta coefficient: {weights[2]:.3f}\")\n",
    "print(f\"  theta_dot coefficient: {weights[3]:.3f}\")\n",
    "print(f\"\\nAction = {weights[0]:.2f}*x + {weights[1]:.2f}*x_dot + {weights[2]:.2f}*theta + {weights[3]:.2f}*theta_dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a successful episode and animate it\n",
    "np.random.seed(123)\n",
    "states, actions, rewards, _ = collect_episode(env, best_policy)\n",
    "\n",
    "history = {\n",
    "    'states': np.array(states),\n",
    "    'actions': np.array(actions),\n",
    "    'rewards': np.array(rewards)\n",
    "}\n",
    "\n",
    "env_params = {\n",
    "    'pole_length': env.pole_length,\n",
    "    'x_threshold': env.x_threshold\n",
    "}\n",
    "\n",
    "print(f\"Episode length: {len(rewards)} steps\")\n",
    "print(\"Creating animation...\")\n",
    "\n",
    "# Create animation (subsample for speed)\n",
    "subsample = 3  # Show every 3rd frame\n",
    "subsampled_history = {\n",
    "    'states': history['states'][::subsample],\n",
    "    'actions': history['actions'][::subsample] if len(history['actions']) > 0 else [],\n",
    "    'rewards': history['rewards'][::subsample] if len(history['rewards']) > 0 else []\n",
    "}\n",
    "\n",
    "anim = animate_pendulum(subsampled_history, env_params, interval=50)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Different Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train policies for different environments\n",
    "environments = {\n",
    "    'Easy (long pole)': InvertedPendulumEnv(pole_length=1.0),\n",
    "    'Normal': InvertedPendulumEnv(),\n",
    "    'Hard (short pole)': InvertedPendulumEnv(pole_length=0.3),\n",
    "    'Very Hard': InvertedPendulumEnv(pole_length=0.25, force_mag=5.0)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, env in environments.items():\n",
    "    print(f\"\\nTraining for {name}...\")\n",
    "    policy = LinearPolicy()\n",
    "    \n",
    "    train_result = train_policy(\n",
    "        env, policy,\n",
    "        algorithm='evolutionary',\n",
    "        n_iterations=100,\n",
    "        population_size=25,\n",
    "        noise_scale=0.4,\n",
    "        n_episodes_per_eval=5,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    eval_result = evaluate_policy(env, policy, n_episodes=50)\n",
    "    results[name] = {\n",
    "        'policy': policy,\n",
    "        'train': train_result,\n",
    "        'eval': eval_result\n",
    "    }\n",
    "    \n",
    "    print(f\"  Mean reward: {eval_result['mean_reward']:.1f} / {env.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training curves\n",
    "for name, result in results.items():\n",
    "    axes[0].plot(result['train']['reward_history'], label=name)\n",
    "axes[0].set_xlabel('Generation')\n",
    "axes[0].set_ylabel('Elite Mean Reward')\n",
    "axes[0].set_title('Training Progress')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance\n",
    "names = list(results.keys())\n",
    "means = [results[n]['eval']['mean_reward'] for n in names]\n",
    "stds = [results[n]['eval']['std_reward'] for n in names]\n",
    "x = range(len(names))\n",
    "\n",
    "bars = axes[1].bar(x, means, yerr=stds, capsize=5)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(names, rotation=15)\n",
    "axes[1].set_ylabel('Mean Reward')\n",
    "axes[1].set_title('Final Performance')\n",
    "axes[1].axhline(y=500, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze the Learned Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at what the trained policy does in detail\n",
    "env = InvertedPendulumEnv()\n",
    "\n",
    "# Collect episode data\n",
    "states, actions, rewards, _ = collect_episode(env, best_policy)\n",
    "history = {\n",
    "    'states': np.array(states),\n",
    "    'actions': np.array(actions),\n",
    "    'rewards': np.array(rewards)\n",
    "}\n",
    "\n",
    "# Full trajectory plot\n",
    "fig = plot_trajectory(history, title='Trained Policy Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the relationship between state and action\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "state_names = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Angular Velocity']\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes.flat, state_names)):\n",
    "    states_i = history['states'][:-1, i]  # Exclude last (no action)\n",
    "    ax.scatter(states_i, history['actions'], alpha=0.3, s=10)\n",
    "    ax.set_xlabel(name)\n",
    "    ax.set_ylabel('Action (Force)')\n",
    "    \n",
    "    # Fit line\n",
    "    z = np.polyfit(states_i, history['actions'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(states_i.min(), states_i.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r-', linewidth=2, label=f'slope={z[0]:.1f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('How Action Depends on Each State Variable')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy analysis\n",
    "env = InvertedPendulumEnv()\n",
    "state = env.reset(seed=789)\n",
    "\n",
    "energies = {'kinetic': [], 'potential': [], 'total': []}\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    energy = env.get_energy()\n",
    "    for key in energies:\n",
    "        energies[key].append(energy[key])\n",
    "    \n",
    "    action = best_policy.get_action(state)\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "# Plot energy\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "steps = range(len(energies['kinetic']))\n",
    "\n",
    "ax.plot(steps, energies['kinetic'], 'r-', label='Kinetic', alpha=0.7)\n",
    "ax.plot(steps, energies['potential'], 'b-', label='Potential', alpha=0.7)\n",
    "ax.plot(steps, energies['total'], 'g--', label='Total', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Energy (J)')\n",
    "ax.set_title('System Energy During Controlled Episode')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: The controller adds/removes energy to keep the pole balanced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "In this series of notebooks, we covered:\n",
    "\n",
    "1. **RL Fundamentals**: Agent, environment, state, action, reward, policy\n",
    "2. **Inverted Pendulum Physics**: Equations of motion, unstable equilibrium\n",
    "3. **Environment Design**: Reset/step interface, customization, history tracking\n",
    "4. **Policy Types**: Random, linear, neural network\n",
    "5. **Training Algorithms**: Random search, hill climbing, evolution strategies\n",
    "6. **Visualization**: Trajectories, animations, policy surfaces\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Simple environments are great for learning RL concepts\n",
    "- Linear policies can be surprisingly effective\n",
    "- Evolution strategies work well for policy optimization\n",
    "- Visualization helps understand what agents learn\n",
    "\n",
    "### Next Steps in Your RL Journey\n",
    "\n",
    "1. **Policy Gradients**: Learn REINFORCE and actor-critic methods\n",
    "2. **Value Functions**: Q-learning, DQN, TD learning\n",
    "3. **Continuous Control**: PPO, SAC, DDPG\n",
    "4. **Harder Environments**: MuJoCo, Atari, robotics\n",
    "5. **Advanced Topics**: Model-based RL, hierarchical RL, multi-agent RL\n",
    "\n",
    "### Recommended Resources\n",
    "\n",
    "- Sutton & Barto: \"Reinforcement Learning: An Introduction\"\n",
    "- OpenAI Spinning Up: spinningup.openai.com\n",
    "- Deep RL Course by HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final celebration - watch your best policy!\n",
    "print(\"Congratulations on completing the RL tutorial!\")\n",
    "print(\"\\nYour trained policy achieved:\")\n",
    "\n",
    "env = InvertedPendulumEnv()\n",
    "final_eval = evaluate_policy(env, best_policy, n_episodes=100)\n",
    "\n",
    "print(f\"  Mean reward: {final_eval['mean_reward']:.1f} / 500\")\n",
    "print(f\"  Success rate: {100 * sum(r >= 490 for r in final_eval['episode_rewards']) / 100:.0f}%\")\n",
    "print(f\"\\nYou've learned the fundamentals of reinforcement learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Challenge Yourself!\n",
    "\n",
    "Try these extensions:\n",
    "\n",
    "1. **Double Inverted Pendulum**: Add a second pole segment\n",
    "2. **Swing-Up Task**: Start with pole hanging down, swing it up\n",
    "3. **Moving Target**: Keep cart at a changing target position\n",
    "4. **Noisy Observations**: Add sensor noise to states\n",
    "5. **Delayed Rewards**: Sparse rewards only at episode end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
