{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Training Policies\n",
    "\n",
    "Now comes the exciting part - **learning**! In this notebook, we'll train policies to balance the pendulum automatically, without manually specifying the weights.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. The training objective\n",
    "2. Random search (simplest learning)\n",
    "3. Hill climbing\n",
    "4. Evolution strategies\n",
    "5. Comparing algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environments import InvertedPendulumEnv\n",
    "from src.policies import LinearPolicy, NeuralNetworkPolicy\n",
    "from src.utils import train_policy, evaluate_policy, plot_training_progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Training Objective\n",
    "\n",
    "Our goal is to find policy parameters $\\theta$ that maximize expected return:\n",
    "\n",
    "$$\\theta^* = \\arg\\max_\\theta \\mathbb{E}\\left[ \\sum_{t=0}^{T} r_t \\right]$$\n",
    "\n",
    "In simpler terms:\n",
    "- Run the policy in the environment\n",
    "- Calculate total reward\n",
    "- Adjust parameters to get higher reward\n",
    "- Repeat!\n",
    "\n",
    "### Key Challenges\n",
    "- We don't have gradients (unlike supervised learning)\n",
    "- Rewards are noisy (stochastic environment)\n",
    "- Credit assignment: which actions led to good outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how noisy the reward signal is\n",
    "env = InvertedPendulumEnv()\n",
    "policy = LinearPolicy(weights=np.array([0, 0, 10, 3]))\n",
    "\n",
    "rewards = []\n",
    "for i in range(100):\n",
    "    result = evaluate_policy(env, policy, n_episodes=1)\n",
    "    rewards.append(result['mean_reward'])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Same Policy, Different Outcomes')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rewards, bins=20)\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Reward Distribution\\nmean={np.mean(rewards):.1f}, std={np.std(rewards):.1f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Even the same policy gives different rewards each episode!\")\n",
    "print(\"This is why we average over multiple episodes when evaluating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Search\n",
    "\n",
    "The simplest possible learning algorithm:\n",
    "\n",
    "1. Generate random parameters\n",
    "2. Evaluate performance\n",
    "3. Keep the best\n",
    "4. Repeat\n",
    "\n",
    "It's surprisingly effective for low-dimensional problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with random search\n",
    "env = InvertedPendulumEnv()\n",
    "policy = LinearPolicy()  # Start with zeros\n",
    "\n",
    "print(\"Training with Random Search...\")\n",
    "print(f\"Initial weights: {policy.get_flat_params()}\\n\")\n",
    "\n",
    "result = train_policy(\n",
    "    env, policy,\n",
    "    algorithm='random_search',\n",
    "    n_iterations=100,\n",
    "    noise_scale=2.0,  # Range of random values\n",
    "    n_episodes_per_eval=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest reward: {result['best_reward']:.1f}\")\n",
    "print(f\"Best weights: {result['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(result['reward_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Random Search Training Progress')\n",
    "plt.axhline(y=500, color='r', linestyle='--', label='Maximum')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained policy\n",
    "eval_result = evaluate_policy(env, policy, n_episodes=50, verbose=True)\n",
    "\n",
    "print(f\"\\nTrained Policy Performance:\")\n",
    "print(f\"  Mean reward: {eval_result['mean_reward']:.1f} ± {eval_result['std_reward']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hill Climbing\n",
    "\n",
    "A slightly smarter approach:\n",
    "\n",
    "1. Start with current parameters\n",
    "2. Add small random perturbation\n",
    "3. If better, keep new parameters\n",
    "4. Repeat\n",
    "\n",
    "This is like walking uphill in the dark - only take steps that go up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with hill climbing\n",
    "env = InvertedPendulumEnv()\n",
    "policy = LinearPolicy()\n",
    "\n",
    "print(\"Training with Hill Climbing...\\n\")\n",
    "\n",
    "result_hc = train_policy(\n",
    "    env, policy,\n",
    "    algorithm='hill_climbing',\n",
    "    n_iterations=200,\n",
    "    noise_scale=0.5,  # Perturbation size\n",
    "    n_episodes_per_eval=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest reward: {result_hc['best_reward']:.1f}\")\n",
    "print(f\"Best weights: {result_hc['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hill climbing progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(result_hc['reward_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Best Reward So Far')\n",
    "plt.title('Hill Climbing Training Progress')\n",
    "plt.axhline(y=500, color='r', linestyle='--', label='Maximum')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Progress is monotonic (never goes down)!\")\n",
    "print(\"But it can get stuck in local optima.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evolution Strategies\n",
    "\n",
    "A more sophisticated approach inspired by natural evolution:\n",
    "\n",
    "1. Create a population of perturbed parameters\n",
    "2. Evaluate fitness of each\n",
    "3. Move toward the best performers (weighted average)\n",
    "4. Repeat\n",
    "\n",
    "This is more robust and can escape local optima!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with evolution strategies\n",
    "env = InvertedPendulumEnv()\n",
    "policy = LinearPolicy()\n",
    "\n",
    "print(\"Training with Evolution Strategies...\\n\")\n",
    "\n",
    "result_es = train_policy(\n",
    "    env, policy,\n",
    "    algorithm='evolutionary',\n",
    "    n_iterations=50,\n",
    "    population_size=20,  # Number of variants to try\n",
    "    elite_frac=0.2,  # Keep top 20%\n",
    "    noise_scale=0.5,\n",
    "    n_episodes_per_eval=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest reward: {result_es['best_reward']:.1f}\")\n",
    "print(f\"Best weights: {result_es['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolution strategies progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(result_es['reward_history'])\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Elite Mean Reward')\n",
    "plt.title('Evolution Strategies Training Progress')\n",
    "plt.axhline(y=500, color='r', linestyle='--', label='Maximum')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Algorithms\n",
    "\n",
    "Let's run all three algorithms multiple times and compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each algorithm multiple times\n",
    "n_runs = 5\n",
    "n_iterations = 100\n",
    "\n",
    "algorithms = ['random_search', 'hill_climbing', 'evolutionary']\n",
    "all_results = {alg: [] for alg in algorithms}\n",
    "\n",
    "print(f\"Running each algorithm {n_runs} times...\\n\")\n",
    "\n",
    "for alg in algorithms:\n",
    "    print(f\"\\n{alg}:\")\n",
    "    for run in range(n_runs):\n",
    "        env = InvertedPendulumEnv()\n",
    "        policy = LinearPolicy()\n",
    "        \n",
    "        result = train_policy(\n",
    "            env, policy,\n",
    "            algorithm=alg,\n",
    "            n_iterations=n_iterations,\n",
    "            population_size=15,\n",
    "            noise_scale=0.5,\n",
    "            n_episodes_per_eval=3,\n",
    "            verbose=False,\n",
    "            seed=run\n",
    "        )\n",
    "        \n",
    "        # Evaluate final policy\n",
    "        eval_result = evaluate_policy(env, policy, n_episodes=20)\n",
    "        all_results[alg].append(eval_result['mean_reward'])\n",
    "        print(f\"  Run {run+1}: {eval_result['mean_reward']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Box plot\n",
    "data = [all_results[alg] for alg in algorithms]\n",
    "ax1.boxplot(data, labels=['Random', 'Hill Climb', 'Evolution'])\n",
    "ax1.set_ylabel('Final Mean Reward')\n",
    "ax1.set_title('Algorithm Comparison')\n",
    "ax1.axhline(y=500, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Bar plot with error bars\n",
    "means = [np.mean(all_results[alg]) for alg in algorithms]\n",
    "stds = [np.std(all_results[alg]) for alg in algorithms]\n",
    "x = range(len(algorithms))\n",
    "ax2.bar(x, means, yerr=stds, capsize=5)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['Random', 'Hill Climb', 'Evolution'])\n",
    "ax2.set_ylabel('Mean Reward ± Std')\n",
    "ax2.set_title('Average Performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary:\")\n",
    "for alg in algorithms:\n",
    "    rewards = all_results[alg]\n",
    "    print(f\"  {alg:15s}: {np.mean(rewards):.1f} ± {np.std(rewards):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a Neural Network Policy\n",
    "\n",
    "Can we train a neural network with these same algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a neural network with evolution strategies\n",
    "env = InvertedPendulumEnv()\n",
    "nn_policy = NeuralNetworkPolicy(hidden_sizes=[16, 16])  # Smaller network\n",
    "\n",
    "print(f\"Neural network has {nn_policy.get_num_params()} parameters\")\n",
    "print(\"(vs 5 for linear policy)\\n\")\n",
    "\n",
    "print(\"Training neural network with Evolution Strategies...\\n\")\n",
    "\n",
    "result_nn = train_policy(\n",
    "    env, nn_policy,\n",
    "    algorithm='evolutionary',\n",
    "    n_iterations=100,\n",
    "    population_size=30,\n",
    "    noise_scale=0.3,\n",
    "    n_episodes_per_eval=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest reward: {result_nn['best_reward']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained neural network\n",
    "eval_result = evaluate_policy(env, nn_policy, n_episodes=50)\n",
    "\n",
    "print(f\"Trained Neural Network Performance:\")\n",
    "print(f\"  Mean reward: {eval_result['mean_reward']:.1f} ± {eval_result['std_reward']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained neural network's policy surface\n",
    "from src.utils.visualization import plot_policy_surface\n",
    "\n",
    "fig = plot_policy_surface(\n",
    "    nn_policy,\n",
    "    state_ranges={'theta': (-0.3, 0.3), 'theta_dot': (-2, 2)},\n",
    "    fixed_states={'x': 0, 'x_dot': 0}\n",
    ")\n",
    "plt.suptitle('Trained Neural Network Policy', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: The trained network learned a sensible policy!\")\n",
    "print(\"Positive theta -> positive action (push right)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Hyperparameter Tuning\n",
    "Try different hyperparameters for evolution strategies (population size, elite fraction, noise scale). What works best?\n",
    "\n",
    "### Exercise 2: Longer Training\n",
    "Train for more iterations. Does performance keep improving? When does it plateau?\n",
    "\n",
    "### Exercise 3: Harder Environment\n",
    "Try training on a harder environment (shorter pole, less force). Can the algorithms still find good policies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Train on harder environment\n",
    "hard_env = InvertedPendulumEnv(\n",
    "    pole_length=0.3,  # Shorter pole\n",
    "    force_mag=8.0,    # Less force\n",
    "    theta_threshold=0.15  # Stricter angle\n",
    ")\n",
    "\n",
    "policy = LinearPolicy()\n",
    "\n",
    "print(\"Training on harder environment...\\n\")\n",
    "\n",
    "result_hard = train_policy(\n",
    "    hard_env, policy,\n",
    "    algorithm='evolutionary',\n",
    "    n_iterations=100,\n",
    "    population_size=30,\n",
    "    noise_scale=0.5,\n",
    "    n_episodes_per_eval=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_result = evaluate_policy(hard_env, policy, n_episodes=50)\n",
    "print(f\"\\nHard Environment Performance:\")\n",
    "print(f\"  Mean reward: {eval_result['mean_reward']:.1f} ± {eval_result['std_reward']:.1f}\")\n",
    "print(f\"  Max possible: {hard_env.max_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "- **Training objective**: Maximize expected cumulative reward\n",
    "- **Random search**: Simple but effective baseline\n",
    "- **Hill climbing**: Only accepts improvements, can get stuck\n",
    "- **Evolution strategies**: Uses population, more robust\n",
    "- Neural networks need more iterations but can learn too!\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. Even simple algorithms can learn good policies\n",
    "2. Evolution strategies tends to be most reliable\n",
    "3. More parameters = more iterations needed\n",
    "4. Always evaluate with multiple episodes (reduce noise)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the final notebook, we'll run experiments, visualize results, and explore what we've learned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
