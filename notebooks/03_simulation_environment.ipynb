{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: The Simulation Environment\n",
    "\n",
    "In this notebook, we'll take a deep dive into our simulation environment. Understanding the environment interface is crucial for RL - it's what your agent will interact with!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. The environment API (reset, step, etc.)\n",
    "2. How to customize environment parameters\n",
    "3. Understanding rewards and termination\n",
    "4. Collecting and storing episode data\n",
    "5. Visualizing episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environments import InvertedPendulumEnv\n",
    "from src.utils import plot_trajectory, animate_pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Environment API\n",
    "\n",
    "Our environment follows the standard RL environment interface (similar to OpenAI Gym):\n",
    "\n",
    "### Key Methods\n",
    "\n",
    "| Method | Description | Returns |\n",
    "|--------|-------------|--------|\n",
    "| `reset()` | Start a new episode | Initial state |\n",
    "| `step(action)` | Take one action | (state, reward, done, info) |\n",
    "| `get_history()` | Get episode data | Dict of arrays |\n",
    "\n",
    "This is the interface you'll use in all RL algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = InvertedPendulumEnv()\n",
    "\n",
    "# See what's inside\n",
    "print(\"Environment Parameters:\")\n",
    "print(f\"  Gravity: {env.gravity} m/s²\")\n",
    "print(f\"  Cart mass: {env.cart_mass} kg\")\n",
    "print(f\"  Pole mass: {env.pole_mass} kg\")\n",
    "print(f\"  Pole length: {env.pole_length} m (half-length)\")\n",
    "print(f\"  Max force: {env.force_mag} N\")\n",
    "print(f\"  Time step: {env.tau} s\")\n",
    "print(f\"  Max steps: {env.max_steps}\")\n",
    "print(f\"\\nTermination thresholds:\")\n",
    "print(f\"  Angle: ±{np.degrees(env.theta_threshold):.1f}°\")\n",
    "print(f\"  Position: ±{env.x_threshold} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "state = env.reset(seed=42)\n",
    "\n",
    "print(\"After reset():\")\n",
    "print(f\"  State: {state}\")\n",
    "print(f\"  State shape: {state.shape}\")\n",
    "print(f\"\\n  [x, x_dot, theta, theta_dot]\")\n",
    "print(f\"  Position: {state[0]:.4f} m\")\n",
    "print(f\"  Velocity: {state[1]:.4f} m/s\")\n",
    "print(f\"  Angle: {np.degrees(state[2]):.2f}°\")\n",
    "print(f\"  Angular vel: {state[3]:.4f} rad/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a step\n",
    "action = 5.0  # Push right with 5 Newtons\n",
    "next_state, reward, done, info = env.step(action)\n",
    "\n",
    "print(f\"After step(action={action}):\")\n",
    "print(f\"\\n  Next state: {next_state}\")\n",
    "print(f\"  Reward: {reward}\")\n",
    "print(f\"  Done: {done}\")\n",
    "print(f\"\\n  Info dict:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Customizing the Environment\n",
    "\n",
    "You can modify the physics parameters to create different challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy environment (longer pole = easier to balance)\n",
    "easy_env = InvertedPendulumEnv(\n",
    "    pole_length=1.0,  # Longer pole\n",
    "    theta_threshold=0.4,  # More forgiving angle\n",
    "    max_steps=1000\n",
    ")\n",
    "\n",
    "# Hard environment\n",
    "hard_env = InvertedPendulumEnv(\n",
    "    pole_length=0.25,  # Shorter pole\n",
    "    theta_threshold=0.1,  # Strict angle requirement\n",
    "    force_mag=5.0,  # Less force available\n",
    "    max_steps=500\n",
    ")\n",
    "\n",
    "# Moon environment!\n",
    "moon_env = InvertedPendulumEnv(\n",
    "    gravity=1.62,  # Moon gravity\n",
    "    max_steps=500\n",
    ")\n",
    "\n",
    "print(\"Created three custom environments!\")\n",
    "print(f\"Easy: pole_length={easy_env.pole_length}m\")\n",
    "print(f\"Hard: pole_length={hard_env.pole_length}m\")\n",
    "print(f\"Moon: gravity={moon_env.gravity}m/s²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare how long each balances with same initial condition and no control\n",
    "envs = [\n",
    "    (easy_env, \"Easy (long pole)\"),\n",
    "    (env, \"Default\"),\n",
    "    (hard_env, \"Hard (short pole)\"),\n",
    "    (moon_env, \"Moon gravity\")\n",
    "]\n",
    "\n",
    "initial_state = [0, 0, 0.05, 0]\n",
    "\n",
    "print(\"Steps to fall with no control (from θ=0.05 rad):\\n\")\n",
    "for e, name in envs:\n",
    "    e.reset(initial_state=initial_state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        _, _, done, _ = e.step(0)\n",
    "    print(f\"  {name}: {e.steps} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Rewards\n",
    "\n",
    "The reward function defines what we want the agent to achieve:\n",
    "\n",
    "### Current Reward Function\n",
    "- **+1** for each step the pole stays balanced\n",
    "- **0** when episode terminates\n",
    "\n",
    "This encourages the agent to keep the pole balanced as long as possible!\n",
    "\n",
    "### Alternative Reward Functions\n",
    "You could design different rewards:\n",
    "- Penalize large angles\n",
    "- Penalize large velocities\n",
    "- Penalize control effort (energy efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an episode and track rewards\n",
    "env = InvertedPendulumEnv()\n",
    "state = env.reset(seed=123)\n",
    "\n",
    "# Simple proportional control\n",
    "rewards = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Simple control: push in direction of lean\n",
    "    theta = state[2]\n",
    "    action = 10 * theta + 2 * state[3]  # PD control\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "print(f\"Episode length: {len(rewards)} steps\")\n",
    "print(f\"Total reward: {sum(rewards)}\")\n",
    "print(f\"\\nTermination reason:\")\n",
    "if info['terminated_by_time']:\n",
    "    print(\"  Success! Reached maximum steps.\")\n",
    "else:\n",
    "    print(\"  Failed - pole fell or cart off track\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collecting Episode Data\n",
    "\n",
    "For learning and visualization, we need to record what happened during an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The environment automatically records history\n",
    "env = InvertedPendulumEnv()\n",
    "state = env.reset(seed=456)\n",
    "\n",
    "# Run episode with simple control\n",
    "done = False\n",
    "while not done:\n",
    "    action = 8 * state[2] + 1.5 * state[3]\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "# Get the history\n",
    "history = env.get_history()\n",
    "\n",
    "print(\"Episode history:\")\n",
    "print(f\"  States shape: {history['states'].shape}\")\n",
    "print(f\"  Actions shape: {history['actions'].shape}\")\n",
    "print(f\"  Rewards shape: {history['rewards'].shape}\")\n",
    "\n",
    "print(f\"\\nFirst 5 states:\")\n",
    "print(history['states'][:5])\n",
    "\n",
    "print(f\"\\nFirst 5 actions:\")\n",
    "print(history['actions'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Episodes\n",
    "\n",
    "Let's use our visualization tools to understand what's happening!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trajectory\n",
    "fig = plot_trajectory(history, title=\"Episode with Simple PD Control\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate the episode (this creates an animation object)\n",
    "# Note: Animation display depends on your Jupyter setup\n",
    "from IPython.display import HTML\n",
    "\n",
    "env_params = {\n",
    "    'pole_length': env.pole_length,\n",
    "    'x_threshold': env.x_threshold\n",
    "}\n",
    "\n",
    "anim = animate_pendulum(history, env_params, interval=50)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at the phase portrait\n",
    "from src.utils.visualization import plot_phase_portrait\n",
    "\n",
    "fig = plot_phase_portrait(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Here's a complete example of running multiple episodes and collecting statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, control_fn, seed=None):\n",
    "    \"\"\"Run one episode with a given control function.\"\"\"\n",
    "    state = env.reset(seed=seed)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = control_fn(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    return total_reward, env.get_history(), info\n",
    "\n",
    "# Define different control strategies\n",
    "def no_control(state):\n",
    "    return 0\n",
    "\n",
    "def random_control(state):\n",
    "    return np.random.uniform(-10, 10)\n",
    "\n",
    "def proportional_control(state):\n",
    "    return 10 * state[2]  # P control\n",
    "\n",
    "def pd_control(state):\n",
    "    return 10 * state[2] + 3 * state[3]  # PD control\n",
    "\n",
    "# Compare strategies\n",
    "env = InvertedPendulumEnv()\n",
    "n_episodes = 20\n",
    "\n",
    "strategies = [\n",
    "    (no_control, \"No Control\"),\n",
    "    (random_control, \"Random\"),\n",
    "    (proportional_control, \"P Control\"),\n",
    "    (pd_control, \"PD Control\")\n",
    "]\n",
    "\n",
    "print(f\"Average reward over {n_episodes} episodes:\\n\")\n",
    "\n",
    "for control_fn, name in strategies:\n",
    "    rewards = []\n",
    "    for i in range(n_episodes):\n",
    "        reward, _, _ = run_episode(env, control_fn, seed=i)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    print(f\"  {name:15s}: {np.mean(rewards):6.1f} ± {np.std(rewards):5.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Custom Environment\n",
    "Create a \"Jupiter\" environment with high gravity (24.79 m/s²) and test how well PD control works.\n",
    "\n",
    "### Exercise 2: Custom Reward\n",
    "Modify the episode runner to compute a custom reward that penalizes large angles.\n",
    "\n",
    "### Exercise 3: Find Good Control Gains\n",
    "Experiment with different values for the P and D gains in PD control. Can you find values that work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Experiment with PD gains\n",
    "def make_pd_controller(kp, kd):\n",
    "    \"\"\"Create a PD controller with given gains.\"\"\"\n",
    "    def controller(state):\n",
    "        return kp * state[2] + kd * state[3]\n",
    "    return controller\n",
    "\n",
    "# Try different gain combinations\n",
    "env = InvertedPendulumEnv()\n",
    "gain_pairs = [\n",
    "    (5, 1), (10, 1), (10, 3), (10, 5),\n",
    "    (15, 3), (20, 5), (20, 10)\n",
    "]\n",
    "\n",
    "print(\"Testing different PD gains (Kp, Kd):\\n\")\n",
    "\n",
    "best_reward = 0\n",
    "best_gains = None\n",
    "\n",
    "for kp, kd in gain_pairs:\n",
    "    controller = make_pd_controller(kp, kd)\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        reward, _, _ = run_episode(env, controller, seed=i)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    mean_reward = np.mean(rewards)\n",
    "    print(f\"  Kp={kp:2d}, Kd={kd:2d}: {mean_reward:6.1f}\")\n",
    "    \n",
    "    if mean_reward > best_reward:\n",
    "        best_reward = mean_reward\n",
    "        best_gains = (kp, kd)\n",
    "\n",
    "print(f\"\\nBest gains: Kp={best_gains[0]}, Kd={best_gains[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "- **Environment API**: `reset()`, `step()`, `get_history()`\n",
    "- **Customization**: Modify physics parameters for different challenges\n",
    "- **Rewards**: +1 per step encourages balancing\n",
    "- **Data collection**: The environment records states, actions, rewards\n",
    "- **Visualization**: Plot trajectories and animate episodes\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that we understand the environment, let's learn about **policies** - the agent's decision-making strategy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
