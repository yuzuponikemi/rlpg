{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Introduction to Reinforcement Learning\n",
    "\n",
    "Welcome to this hands-on tutorial on Reinforcement Learning (RL)! In this series of notebooks, we'll learn the fundamental concepts of RL by solving the classic **Inverted Pendulum** (Cart-Pole) problem.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. What is Reinforcement Learning?\n",
    "2. Key concepts: Agent, Environment, State, Action, Reward\n",
    "3. The RL loop\n",
    "4. Why the Inverted Pendulum is a great learning problem\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python programming\n",
    "- Basic understanding of NumPy\n",
    "- High school physics (helpful but not required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**.\n",
    "\n",
    "Unlike supervised learning (where we have labeled examples) or unsupervised learning (where we find patterns), RL learns from **experience** through trial and error.\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "- A robot learning to walk\n",
    "- An AI playing video games\n",
    "- A thermostat learning your preferences\n",
    "- Self-driving cars\n",
    "\n",
    "### The Key Idea\n",
    "\n",
    "The agent takes **actions** in the environment, receives **rewards** (or penalties), and learns to maximize the total reward over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Concepts\n",
    "\n",
    "Let's define the fundamental components of an RL system:\n",
    "\n",
    "### Agent\n",
    "The learner and decision-maker. In our case, it's the controller for the cart.\n",
    "\n",
    "### Environment\n",
    "Everything the agent interacts with. For us, it's the cart-pole system with physics.\n",
    "\n",
    "### State (s)\n",
    "A description of the current situation. For the pendulum:\n",
    "- Cart position (x)\n",
    "- Cart velocity (ẋ)\n",
    "- Pole angle (θ)\n",
    "- Pole angular velocity (θ̇)\n",
    "\n",
    "### Action (a)\n",
    "What the agent can do. For us: push the cart left or right (apply force).\n",
    "\n",
    "### Reward (r)\n",
    "Feedback signal. For the pendulum: +1 for each step the pole stays balanced.\n",
    "\n",
    "### Policy (π)\n",
    "The agent's strategy: a mapping from states to actions. This is what we want to learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the RL loop\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# Agent box\n",
    "agent = patches.FancyBboxPatch((1, 4), 2, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                                facecolor='lightblue', edgecolor='black')\n",
    "ax.add_patch(agent)\n",
    "ax.text(2, 4.75, 'Agent\\n(Policy)', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Environment box\n",
    "env = patches.FancyBboxPatch((6, 4), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='lightgreen', edgecolor='black')\n",
    "ax.add_patch(env)\n",
    "ax.text(7.5, 4.75, 'Environment\\n(Pendulum)', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Arrows\n",
    "# Action arrow\n",
    "ax.annotate('', xy=(6, 5.2), xytext=(3, 5.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax.text(4.5, 5.5, 'Action (Force)', ha='center', fontsize=10, color='red')\n",
    "\n",
    "# State arrow\n",
    "ax.annotate('', xy=(3, 4.3), xytext=(6, 4.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "ax.text(4.5, 3.8, 'State', ha='center', fontsize=10, color='blue')\n",
    "\n",
    "# Reward arrow\n",
    "ax.annotate('', xy=(2, 4), xytext=(7.5, 2),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2,\n",
    "                          connectionstyle='arc3,rad=0.3'))\n",
    "ax.text(5, 2.3, 'Reward', ha='center', fontsize=10, color='green')\n",
    "\n",
    "plt.title('The Reinforcement Learning Loop', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The RL Loop\n",
    "\n",
    "At each time step t:\n",
    "\n",
    "1. Agent observes state $s_t$\n",
    "2. Agent selects action $a_t = \\pi(s_t)$\n",
    "3. Environment transitions to state $s_{t+1}$\n",
    "4. Agent receives reward $r_t$\n",
    "5. Repeat!\n",
    "\n",
    "The **goal** is to find a policy π that maximizes the **cumulative reward**:\n",
    "\n",
    "$$G = \\sum_{t=0}^{T} r_t$$\n",
    "\n",
    "Or with discounting (to prefer sooner rewards):\n",
    "\n",
    "$$G = \\sum_{t=0}^{\\infty} \\gamma^t r_t$$\n",
    "\n",
    "where $\\gamma \\in [0, 1]$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why the Inverted Pendulum?\n",
    "\n",
    "The inverted pendulum is a classic control problem that's perfect for learning RL because:\n",
    "\n",
    "### Simple to Understand\n",
    "- Clear physics (gravity, force, motion)\n",
    "- Easy to visualize\n",
    "- Intuitive goal (keep the pole balanced)\n",
    "\n",
    "### Challenging Enough\n",
    "- Unstable equilibrium (requires active control)\n",
    "- Continuous state space\n",
    "- Quick feedback (episodes are short)\n",
    "\n",
    "### Great for Learning\n",
    "- Simple policies can work (linear controller)\n",
    "- Can scale to neural networks\n",
    "- Fast to train and iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up our environment and see it in action!\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "from src.environments import InvertedPendulumEnv\n",
    "from src.policies import RandomPolicy\n",
    "\n",
    "# Create the environment\n",
    "env = InvertedPendulumEnv()\n",
    "print(env)\n",
    "print(f\"\\nState variables: {env.get_state_labels()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run a single episode with a random policy\n",
    "policy = RandomPolicy()\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset(seed=42)\n",
    "print(f\"Initial state: {state}\")\n",
    "print(f\"  - Cart position: {state[0]:.3f} m\")\n",
    "print(f\"  - Cart velocity: {state[1]:.3f} m/s\")\n",
    "print(f\"  - Pole angle: {np.degrees(state[2]):.2f} degrees\")\n",
    "print(f\"  - Angular velocity: {state[3]:.3f} rad/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the episode\n",
    "total_reward = 0\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    # Agent selects action\n",
    "    action = policy.get_action(state)\n",
    "    \n",
    "    # Environment step\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "\n",
    "print(f\"\\nEpisode finished after {step} steps\")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"\\nTermination reason:\")\n",
    "if info['terminated_by_angle']:\n",
    "    print(f\"  - Pole fell over (angle too large)\")\n",
    "if info['terminated_by_position']:\n",
    "    print(f\"  - Cart went off track\")\n",
    "if info['terminated_by_time']:\n",
    "    print(f\"  - Maximum steps reached (success!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the episode\n",
    "from src.utils import plot_trajectory\n",
    "\n",
    "history = env.get_history()\n",
    "fig = plot_trajectory(history, title=\"Random Policy Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these to solidify your understanding:\n",
    "\n",
    "### Exercise 1: Multiple Episodes\n",
    "Run 10 episodes with the random policy and calculate the average reward.\n",
    "\n",
    "### Exercise 2: Understand the State\n",
    "Modify the code to print the state at each step. Watch how the angle changes!\n",
    "\n",
    "### Exercise 3: Think About Policy\n",
    "If the pole is leaning right (positive theta), what action should you take? Write down your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Use a loop and track rewards\n",
    "\n",
    "rewards = []\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    total = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy.get_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total += reward\n",
    "    rewards.append(total)\n",
    "\n",
    "print(f\"Average reward over 10 episodes: {np.mean(rewards):.1f}\")\n",
    "print(f\"Best episode: {max(rewards):.0f}\")\n",
    "print(f\"Worst episode: {min(rewards):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "- **Reinforcement Learning** is learning from interaction (trial and error)\n",
    "- Key components: **Agent**, **Environment**, **State**, **Action**, **Reward**, **Policy**\n",
    "- The RL loop: observe → act → receive reward → repeat\n",
    "- Goal: find a policy that maximizes cumulative reward\n",
    "- The **inverted pendulum** is our testbed for learning RL\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll dive into the **physics of the inverted pendulum** and understand why balancing it is challenging!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
